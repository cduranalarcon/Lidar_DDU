{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion of Raw lidar data to cloud and precipitation classification\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This Notebook contains the post processing of the lidar data, including the spatial and temporal integration, the system calibration, background correction, SNR estimation, volume depolarization estimation, Cloud masking and hidrometeor clasification. Quick looks and some statistics are generated as well. Results are saved in netCDF format.\n",
    "\n",
    "## Import libraries\n",
    "\n",
    "Generic and own packages stored in \"lib\" are loaded to be used along the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"lib\") # adding \"lib\" path with own packages\n",
    "from Sigma_mol import sigma_mol # reads the radio sounsing and compute extinction coefficient\n",
    "from scipy.interpolate import interp1d # to interpolate modeled variables to lidar heights\n",
    "from lidar_integrating_space_time import Lidar_space_time as lidar_integ #integrates lidar raw data in height and time\n",
    "from fft_denoising import fft_denoising #maybe not used\n",
    "from klett import Klett81b #maybe not used\n",
    "import numpy as np \n",
    "import pylab #plots\n",
    "from DP_simp import DP_simp # Curve simplification\n",
    "from running_mean import running_mean # runing mean\n",
    "from time2epoch import time2epoch #maybe not used?\n",
    "#from cloud_mask_v1 import cloud_mask\n",
    "from netCDF4 import Dataset \n",
    "#from time import sleep\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "from dplots import densplot # make 2D desity plots\n",
    "from Comb_LidarMRR import Comb_LidarMRR3 as Comb_LidarMRR \n",
    "from cloud_mask_v2 import cloud_mask2\n",
    "from sm_paramTOP import sm_paramTOP\n",
    "import matplotlib\n",
    "from copy import copy\n",
    "from BG_corr import BG_corr\n",
    "import time\n",
    "from calendar import timegm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MRR and other parameters\n",
    "\n",
    "Load MRR data during the period of study. It also configures the font format and color maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### output Temporal RESolution\n",
    "TRES = 10 #min. \n",
    "\n",
    "##### output Temporal resolution\n",
    "VRES = 6 #bins, 1bin = 3.8m # try only 1,2,3,6 bins\n",
    "\n",
    "#####Load MRR Data\n",
    "path_MRR = \"I:/PHD/Lidar/Inversion_V2/MRR_Data/\"\n",
    "Ze = np.loadtxt(path_MRR + \"Ze_\"+str(TRES)+\"min.txt\")\n",
    "times_MRR = np.loadtxt(path_MRR + \"times_\"+str(TRES)+\"min.txt\")\n",
    "Height_MRR = np.loadtxt(path_MRR + \"Height.txt\")\n",
    "\n",
    "Zem = np.ma.masked_where(Ze == -9999, Ze)\n",
    "\n",
    "#####Load font format\n",
    "\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "        \n",
    "pylab.rc('font', **font)      \n",
    "\n",
    "#####Color Parameters\n",
    "cmap = pylab.cm.jet\n",
    "bounds = np.linspace(1,3,4)\n",
    "norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
    "bounds2 = np.linspace(0,12,14)\n",
    "norm2 = matplotlib.colors.BoundaryNorm(bounds2, cmap.N)\n",
    "\n",
    "cmap2 = pylab.cm.get_cmap(cmap.name,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Temporal and vertical integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "##### Dates\n",
    "#[ini, end]\n",
    "year0 = [2017, 2017]\n",
    "month0 = [9,12]\n",
    "day0 = [1,31]\n",
    "t0 = time.time()\n",
    "##### Routine\n",
    "for year in range(year0[0],year0[1]+1):\n",
    "    for month in range(month0[0],month0[1]+1):\n",
    "        for day in range(day0[0],day0[1]+1):\n",
    "    \n",
    "            path_out = \"I:/PHD/Lidar/Processing_V3/Signals/\"+str(TRES)+\"min\"+str(VRES)+\"bins/\"\n",
    "            filename1 = path_out+\"Par90/Par90_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "            filename2 = path_out+\"Par10/Par10_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "            filename3 = path_out+\"Per/Per_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "            \n",
    "            filename4 = path_out+\"Nprofiles/Nprofiles_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min\"+\".dat\"\n",
    "            \n",
    "            if os.path.isfile(filename1):\n",
    "                #Par90 = np.loadtxt(filename1)\n",
    "                #Par10 = np.loadtxt(filename2)\n",
    "                #Per   = np.loadtxt(filename3)\n",
    "                #r   = np.loadtxt(path_out+\"R_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\")\n",
    "                #npt = np.loadtxt(filename4)\n",
    "                print str(year)+str(month).zfill(2)+str(day).zfill(2)+\" Data loaded\"\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    mat = lidar_integ(date = str(year)+\".\"+str(month).zfill(2)+\".\"+str(day).zfill(2), space = VRES, timee = TRES, \n",
    "                              path = \"G:/PC_chantal_20190131/MCS6A Data/\")\n",
    "                except:\n",
    "                    continue\n",
    "                    #print \"what\"\n",
    "                print str(year)+str(month).zfill(2)+str(day).zfill(2)+\" Data loaded\"\n",
    "                #sleep(8)\n",
    "                np.savetxt(filename1,mat[0])\n",
    "                np.savetxt(filename2,mat[1])\n",
    "                np.savetxt(filename3,mat[2])\n",
    "                #np.savetxt(path_out+\"R_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\",mat[3])\n",
    "                np.savetxt(filename4,mat[4])\n",
    "                #Par90 = mat[0]\n",
    "                #Par10 = mat[1]\n",
    "                #Per = mat[2]\n",
    "                #npt = mat[4]\n",
    "print \"Elapsed time = \", (time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background correction and SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for count in range(1):\n",
    "    if count > 0: print \"Next iteration\"\n",
    "    ##### Dates\n",
    "    #[ini, end]\n",
    "    year0 = [2017,2017]\n",
    "    month0 = [2,12]\n",
    "    day0 = [1,31]\n",
    "\n",
    "    ##### Routine\n",
    "    for year in range(year0[0],year0[1]+1):\n",
    "        for month in range(month0[0],month0[1]+1):\n",
    "            for day in range(day0[0],day0[1]+1):\n",
    "\n",
    "                path_in = \"I:/PHD/Lidar/Processing_V3/Signals/\"+str(TRES)+\"min\"+str(VRES)+\"bins/\"\n",
    "                filename1 = path_in+\"Par90/Par90_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "                filename2 = path_in+\"Par10/Par10_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "                filename3 = path_in+\"Per/Per_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "                filename4 = path_in+\"Nprofiles/Nprofiles_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min\"+\".dat\"\n",
    "\n",
    "                path_out = \"I:/PHD/Lidar/Processing_V3/Signals/\"+str(TRES)+\"min\"+str(VRES)+\"bins/\"\n",
    "                \n",
    "                filename5 = path_in+\"Par90_bc/Par90bc_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "                filename6 = path_in+\"Par10_bc/Par10bc_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "                filename7 = path_in+\"Per_bc/Perbc_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "\n",
    "                filename8 = path_in+\"SNR/SNR_Par90_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "                filename9 = path_in+\"SNR/SNR_Par10_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "                filename10 = path_in+\"SNR/SNR_Per_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "\n",
    "                filename11 = path_in+\"Background/BG_Par90_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "                filename12 = path_in+\"Background/BG_Par10_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "                filename13 = path_in+\"Background/BG_Per_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "                \n",
    "                if os.path.isfile(filename13):\n",
    "                    #print str(year)+str(month).zfill(2)+str(day).zfill(2)+\" Data ready\"\n",
    "                    continue\n",
    "                \n",
    "                if os.path.isfile(filename1):\n",
    "                    Par90 = np.loadtxt(filename1)\n",
    "                    Par10 = np.loadtxt(filename2)\n",
    "                    Per   = np.loadtxt(filename3)\n",
    "                    r   = np.loadtxt(path_in+\"R_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\")\n",
    "                    npt = np.loadtxt(filename4)\n",
    "\n",
    "                    BG1 = np.zeros(shape = np.shape(Par90)[0])\n",
    "                    BG10 = np.zeros(shape = np.shape(Par90)[0])\n",
    "                    BG2 = np.zeros(shape = np.shape(Par90)[0])\n",
    "\n",
    "                    print str(year)+str(month).zfill(2)+str(day).zfill(2)+\" Data loaded\"\n",
    "                else:\n",
    "                    #print \"File not found\"\n",
    "                    continue\n",
    "\n",
    "                Par_bc = np.zeros(shape = np.shape(Par90))\n",
    "                Par10_bc = np.zeros(shape = np.shape(Par90))\n",
    "                Per_bc = np.zeros(shape = np.shape(Per))\n",
    "                SNR_par = np.zeros(shape = np.shape(Per))\n",
    "                SNR_par10 = np.zeros(shape = np.shape(Per))\n",
    "                SNR_per = np.zeros(shape = np.shape(Per))\n",
    "\n",
    "                for i in range(np.shape(Par90)[0]):\n",
    "                    try:\n",
    "                        BG1[i] = BG_corr(Par90[i,:],r[:],year, month, day,rcf0 = 9,pol = 'parallel')[0][1]\n",
    "                        BG10[i] = BG_corr(Par10[i,:],r[:],year, month, day,rcf0 = 9,pol = 'parallel')[0][1]\n",
    "                        BG2[i] = BG_corr(Per[i,:],r[:],year, month, day,rcf0 = 9,pol = 'perpendicular')[0][1]  \n",
    "                    except:\n",
    "                        continue\n",
    "                        \n",
    "                    if BG1[i] < 0: BG1[i] = 0\n",
    "                    if BG10[i] < 0: BG10[i] = 0\n",
    "                    if BG2[i] < 0: BG2[i] = 0\n",
    "\n",
    "                    if BG1[i] > np.nanmean(Par90[i,-50:]): BG1[i] = np.nanmean(Par90[i,-50:])\n",
    "                    if BG10[i] > np.nanmean(Par10[i,-50:]): BG10[i] = np.nanmean(Par10[i,-50:])\n",
    "                    if BG2[i] > np.nanmean(Per[i,-50:]): BG2[i] = np.nanmean(Per[i,-50:])\n",
    "\n",
    "                    Par_bc[i,:] = Par90[i,:]-BG1[i]\n",
    "                    Par10_bc[i,:] = Par10[i,:]-BG10[i]\n",
    "                    Per_bc[i,:] = Per[i,:]-BG2[i]  \n",
    "\n",
    "                    SNR_par[i,:] = (Par_bc[i,:]*(npt[i]*TRES*6)**0.5)/(Par_bc[i,:]+2*(BG1[i]))**0.5\n",
    "                    SNR_par10[i,:] = (Par10_bc[i,:]*(npt[i]*TRES*6)**0.5)/(Par10_bc[i,:]+2*(BG10[i]))**0.5\n",
    "                    SNR_per[i,:] = (Per_bc[i,:]*(npt[i]*TRES*6)**0.5)/(Per_bc[i,:]+2*(BG2[i]))**0.5\n",
    "\n",
    "                    Par_bc[i,:] = (Par_bc[i,:])*r**2\n",
    "                    Par10_bc[i,:] = (Par10_bc[i,:])*r**2\n",
    "                    Per_bc[i,:] = (Per_bc[i,:])*r**2  \n",
    "\n",
    "                np.savetxt(filename5,Par_bc)\n",
    "                np.savetxt(filename6,Par10_bc)\n",
    "                np.savetxt(filename7,Per_bc)\n",
    "\n",
    "                np.savetxt(filename8,SNR_par)\n",
    "                np.savetxt(filename9,SNR_par10)\n",
    "                np.savetxt(filename10,SNR_per)\n",
    "\n",
    "                np.savetxt(filename11,BG1)\n",
    "                np.savetxt(filename12,BG10)\n",
    "                np.savetxt(filename13,BG2)\n",
    "    #print \"waiting 15 minutes...\"\n",
    "    #time.sleep(15*60)\n",
    "    #print \"iteration = \", count\n",
    "    #month00 = month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duran\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:86: RuntimeWarning: invalid value encountered in log10\n",
      "C:\\Users\\duran\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:71: RuntimeWarning: divide by zero encountered in divide\n",
      "C:\\Users\\duran\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:86: RuntimeWarning: divide by zero encountered in divide\n",
      "C:\\Users\\duran\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:86: RuntimeWarning: divide by zero encountered in log10\n",
      "C:\\Users\\duran\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:86: RuntimeWarning: invalid value encountered in divide\n",
      "C:\\Users\\duran\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:71: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "## load Calibration parameters\n",
    "txt = open(\"I:/PHD/Lidar/Processing_V3/Calibration_sys/CalSys_\"+str(TRES)+\"min_\"+str(VRES)+\"bins_1.csv\",\"r\")\n",
    "CalSys_header = txt.readline()\n",
    "\n",
    "CalSys_time = []\n",
    "CalSys_Cpar = []\n",
    "CalSys_Cper = []\n",
    "CalSys_K = []\n",
    "\n",
    "for l in txt.readlines():\n",
    "    CalSys_time.append(float(l.split(\",\")[1]))\n",
    "    CalSys_Cpar.append(float(l.split(\",\")[3]))\n",
    "    CalSys_Cper.append(float(l.split(\",\")[4]))  \n",
    "    CalSys_K.append(float(l.split(\",\")[5]))  \n",
    "    \n",
    "CalSys_time = np.array(CalSys_time)\n",
    "CalSys_Cpar = np.array(CalSys_Cpar)\n",
    "CalSys_Cper = np.array(CalSys_Cper)\n",
    "CalSys_K    = np.array(CalSys_K)\n",
    "\n",
    "# overlapping function\n",
    "path_in = \"I:/PHD/Lidar/Processing_V3/Signals/\"+str(TRES)+\"min\"+str(VRES)+\"bins/\"\n",
    "\n",
    "#O = np.loadtxt(path_in + \"Overlapping_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\")\n",
    "#Or = np.loadtxt(path_in + \"Overlapping_\"+str(TRES)+\"min_\"+str(VRES)+\"bins_R.dat\")\n",
    "O = np.loadtxt(path_in+\"Overlapping_\"+str(TRES)+\"min_\"+str(VRES)+\"bins_08032017_14UTC2.dat\")\n",
    "Or = np.loadtxt(path_in+\"Overlapping_\"+str(TRES)+\"min_\"+str(VRES)+\"bins_08032017_14UTC_r2.dat\")\n",
    "##### Signal, X and Depolarization calibration\n",
    "year0 = [2017,2017]\n",
    "month0 = [2,8]\n",
    "day0 = [1,31]\n",
    "\n",
    "for year in range(year0[0],year0[1]+1):\n",
    "    for month in range(month0[0],month0[1]+1):\n",
    "        for day in range(day0[0],day0[1]+1):\n",
    "\n",
    "            filename1 = path_in+\"Par90_bc/Par90bc_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "            filename2 = path_in+\"Per_bc/Perbc_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "            path_out = \"I:/PHD/Lidar/Processing_V3/SysCalibrated/\"+str(TRES)+\"min\"+str(VRES)+\"bins/\"\n",
    "            \n",
    "            if (os.path.isdir(path_out+\"Bpar\") == False): os.makedirs(path_out+\"Bpar\")\n",
    "            if (os.path.isdir(path_out+\"Bper\") == False): os.makedirs(path_out+\"Bper\")\n",
    "            if (os.path.isdir(path_out+\"DepRatio\") == False): os.makedirs(path_out+\"DepRatio\")\n",
    "            if (os.path.isdir(path_out+\"X\") == False): os.makedirs(path_out+\"X\")\n",
    "\n",
    "            filename3 = path_out+\"Bpar/Bpar_sc_O_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "            filename4 = path_out+\"Bper/Bper_sc_O_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "            filename5 = path_out+\"DepRatio/DepRatio_sc_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "            filename6 = path_out+\"X/X_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "\n",
    "            if os.path.isfile(filename6):\n",
    "                continue\n",
    "            if os.path.isfile(filename1):\n",
    "                Par = np.loadtxt(filename1)\n",
    "                Per = np.loadtxt(filename2)\n",
    "                r   = np.loadtxt(path_in+\"R_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\")\n",
    "                \n",
    "                utc_time = time.strptime(str(year)+\"-\"+str(month).zfill(2)+\"-\"+str(day).zfill(2)+\" \"+\"00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "                epoch_time = timegm(utc_time)\n",
    "                \n",
    "                pix_cal = np.squeeze(np.where(abs(CalSys_time-epoch_time) == np.min(abs(CalSys_time-epoch_time))))\n",
    "                \n",
    "                Cpar = np.mean(CalSys_Cpar[pix_cal])\n",
    "                Cper = np.mean(CalSys_Cper[pix_cal])\n",
    "                \n",
    "                #Attenuated Backscatter coefficient\n",
    "                Bpar = Par/Cpar\n",
    "                Bper = Per/Cper\n",
    "                \n",
    "                #Linear Depolarization Ratio\n",
    "                DepRatio = (Bper/Bpar)\n",
    "                \n",
    "                #Overlapping Correction\n",
    "                \n",
    "                Bpar_O = np.zeros(shape = np.shape(Par[:,int(11*3./VRES):]))\n",
    "                Bper_O = np.zeros(shape = np.shape(Per[:,int(11*3./VRES):]))\n",
    "                \n",
    "                for j in range(np.shape(Par)[0]):\n",
    "                    Bpar_O[j,:] = Bpar[j,int(11*3./VRES):]*O \n",
    "                    Bper_O[j,:] = Bper[j,int(11*3./VRES):]*O \n",
    "                \n",
    "                #Attenuation between consecutive layers: X\n",
    "                X = np.zeros(shape = np.shape(Bpar_O))\n",
    "\n",
    "                for i in range(15,np.size(Or)-2):\n",
    "                    X[:,i] = np.log10(Bpar_O[:,i]/Bpar_O[:,i+1])\n",
    "                \n",
    "                #save Data\n",
    "                np.savetxt(filename3,Bpar_O)\n",
    "                np.savetxt(filename4,Bper_O)\n",
    "                np.savetxt(filename5,DepRatio)\n",
    "                np.savetxt(filename6,X)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Cloud Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### \n",
    "year0 = [2017,2017]\n",
    "month0 = [2,2]\n",
    "day0 = [1,1]\n",
    "\n",
    "nstd = 3\n",
    "sm= 5\n",
    "dPlot = False\n",
    "check = False\n",
    "slr = 0\n",
    "th = 1.1\n",
    "\n",
    "\n",
    "path_in = \"I:/PHD/Lidar/Processing_V3/SysCalibrated/\"+str(TRES)+\"min\"+str(VRES)+\"bins/\"\n",
    "\n",
    "r = np.loadtxt(path_in + \"Overlapping_\"+str(TRES)+\"min_\"+str(VRES)+\"bins_R.dat\")\n",
    "\n",
    "for year in range(year0[0],year0[1]+1):\n",
    "    for month in range(month0[0],month0[1]+1):\n",
    "        for day in range(day0[0],day0[1]+1):\n",
    "\n",
    "        filename1 = path_in+\"Bpar/Bpar_sc_O_\"+str(year)+str(month).zfill(2)+str(day).zfill(2)+\"_\"+str(TRES)+\"min_\"+str(VRES)+\"bins.dat\"\n",
    "  \n",
    "        if os.path.isfile(filename6):\n",
    "            continue\n",
    "            \n",
    "        if os.path.isfile(filename1):\n",
    "        \n",
    "            Par = np.loadtxt(filename1)\n",
    "            BASES = np.zeros(shape = (np.size(Par[:,0]),20))\n",
    "            TOPS = np.zeros(shape = (np.size(Par[:,0]),20))\n",
    "            Data_mask = np.zeros(shape = np.shape(Par))\n",
    "            \n",
    "            Y2 = Par#Y2[15:]\n",
    "            R = r#_[3:][15:]\n",
    "            AA=running_mean(Y2,sm)#[sm-1:]\n",
    "            rr=R[(sm-1)/2:-(sm-1)/2]#pd.rolling_mean(r,sm)[sm-1:]\n",
    "\n",
    "            pix=np.squeeze(np.where(AA <0))\n",
    "\n",
    "            AAA = np.zeros(shape = np.size(AA)-1)\n",
    "            sum = 0\n",
    "            for i0 in np.linspace(np.size(AA)-2,0,np.size(AA)-1):\n",
    "                i = int(i0)\n",
    "                #print i,AA[i],AA[i+1]\n",
    "                if ((AA[i]>0) & (AA[i+1]>0)):\n",
    "                    sum = sum+1\n",
    "                    #print sum\n",
    "                    AAA[i]=sum\n",
    "                else:\n",
    "                    sum = 0\n",
    "                    AAA[i]=sum    \n",
    "\n",
    "            AAA[1:] = AAA[1:]-AAA[0:-1]\n",
    "\n",
    "            pix2 = np.squeeze(np.where(AAA > 20))\n",
    "\n",
    "            if np.size(pix2)>1:\n",
    "\n",
    "                deltaH = 0\n",
    "                for nlayer in range(np.size(pix2)):\n",
    "                    #print \"nlayer\",nlayer\n",
    "\n",
    "                    rrr = rr[pix2[nlayer]:pix2[nlayer]+int(AAA[pix2[nlayer]])+1]\n",
    "\n",
    "                    pix3=np.squeeze(np.where((R[:-1] >= rrr[0]) & (R[:-1] <rrr[-1])))#r_[3:-1]\n",
    "\n",
    "                    Y3 = Y2[pix3[0]-(sm-1)/2:pix3[-1]+(sm-1)/2+2]\n",
    "                    R3 = R[pix3[0]-(sm-1)/2:pix3[-1]+(sm-1)/2+2]\n",
    "                    #stop\n",
    "                    c_mask, baseH, topH, layers, layers_mask = cloud_mask2(Y3,R3,doPlot = dPlot, th = th,nstd=nstd, sm = sm, check = check)\n",
    "                    #pylab.yscale(\"log\")\n",
    "                    #stop\n",
    "                    Data_mask[int(h),np.squeeze(np.where(r_ == np.nanmin(R3))):np.squeeze(np.where(r_ == np.nanmax(R3)))+1] = 1.\n",
    "                    if np.sum(baseH) != -9999:\n",
    "                        #print baseH,topH\n",
    "                        BASES5[int(h),0+deltaH:np.size(baseH)+deltaH] = baseH\n",
    "                        TOPS5[int(h),0:np.size(topH)+deltaH] = topH\n",
    "                        deltaH = np.size(baseH)\n",
    "\n",
    "            elif np.size(pix2)>=1:\n",
    "\n",
    "                rrr = rr[pix2:pix2+int(AAA[pix2])+1]\n",
    "\n",
    "                pix3=np.squeeze(np.where((R[:-1] >= rrr[0]) & (R[:-1] <rrr[-1]))) #r_[3:-1]\n",
    "\n",
    "                Y3 = Y2[pix3[0]-(sm-1)/2:pix3[-1]+(sm-1)/2+2]\n",
    "                R3 = R[pix3[0]-(sm-1)/2:pix3[-1]+(sm-1)/2+2] #r_\n",
    "\n",
    "                Data_mask[int(h),np.squeeze(np.where(r_ == np.nanmin(R3))):np.squeeze(np.where(r_ == np.nanmax(R3)))+1] = 1.\n",
    "\n",
    "                #fig, ax1 = pylab.subplots(figsize = (8,5))  \n",
    "                #pylab.xticks(range(11))\n",
    "                #pylab.axis([0,10,1,1e6])\n",
    "\n",
    "                #pylab.plot(r_[3:-1],Y2,color = \"gray\")\n",
    "\n",
    "                c_mask, baseH, topH, layers, layers_mask = cloud_mask2(Y3,R3,doPlot = dPlot, th = th,nstd=nstd, sm = sm, check = check)\n",
    "            #pylab.show()\n",
    "                #stop\n",
    "                if np.sum(baseH) != -9999:\n",
    "                    BASES5[int(h),0:np.size(baseH)] = baseH\n",
    "                    TOPS5[int(h),0:np.size(topH)] = topH            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "               \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
